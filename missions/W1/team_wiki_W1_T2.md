# 1주차 2조 팀 위키

1주차 미션을 수행하고 팀원들과 토의한 내용을 정리한 글입니다.

- 2조
  - 박준우
  - 신민경
  - 김태진

---

## 목차

1. [W1M1 - MT Cars 데이터셋 분석](#1-w1m1---mt-cars-데이터셋-분석)
2. [W1M2 - SQL Tutorial](#2-w1m2---sql-tutorial)
3. [W1M3 - ETL 프로세스 구현하기](#3-w1m3---etl-프로세스-구현하기)

---

## 1. W1M1 - MT Cars 데이터셋 분석

### Q1. 이런 데이터셋을 분석해서 얻을 수 있는 경제적 가치는 무엇일까요? 어떤 비즈니스 상황에서 이런 데이터셋을 분석해서 어떤 경제적 가치를 얻을 수 있을까요?

#### 1) 연비 규제 대응 및 탄소 배출권 관리 (ESG)

- **주체**: 글로벌 자동차 제조사 (전략기획 및 R&D 부서)
- **상황**: 글로벌 환경 규제 강화 및 징벌적 과태료 리스크
- **경제적 가치**
  - **규제 비용 절감**: 차량 무게($wt$)와 마력($hp$)이 연비($mpg$)에 미치는 상관계수를 정량화하여, 목표 연비 달성을 위한 최소한의 경량화 수준을 도출한다. 이를 통해 과징금 리스크를 선제적으로 방어한다.
  - **탄소 배출권 수익화**: 규제 기준 이상의 연비 개선 성공 시, 확보한 초과 달성분을 탄소 배출권 시장에 판매하여 직접적인 부가 수익을 창출한다.
  - **R&D 투자 효율 최적화**: 데이터에 기반하여 '연비 효율이 가장 낮은 세그먼트'를 식별하고, 해당 모델의 경량화 소재 도입에 따른 ROI(투자 대비 효율)를 계산함으로써 한정된 연구개발 예산을 전략적으로 배분한다.

#### 2) 제품 포트폴리오 최적화 및 생산 공정 효율화

- **주체**: 자동차 제조사 생산본부 및 기획 부서
- **상황**: 과도한 제품 라인업으로 인한 생산 복잡도 및 비용 증가
- **경제적 가치**
  - **공정 단순화를 통한 비용 절감**: 교차 분석을 통해 시장 선호도가 극히 낮은 조합(예: 3단 기어 & 수동 변속기 등)을 식별하고 라인업에서 제외한다. 이를 통해 생산 라인 교체 횟수를 줄여 가동률을 높이고, 연간 수십억 원 단위의 재고 관리 및 물류 비용을 절감한다.
  - **판매 기회 손실 방지 및 매출 극대화:** 데이터상 선호도가 검증된 인기 조합에 생산 역량을 집중함으로써, 베스트셀러 모델의 출고 대기 시간을 단축하고 판매 적기를 놓쳐 발생하는 기회 비용을 최소화한다.

#### 3) 차량 성능 데이터 분석을 통한 물류·운송 기업의 유지비 절감 및 운영 효율화

- **주체**: 물류, 운송 기업(택배, 화물)
- **상황:** 운임 단가가 제한적인 산업의 특성상 유지 비용 절감이 곧 수익성 개선으로 직결되는 상황
- **경제적 가치**
  - **노선, 업무 특성별 차량 배치 최적화**: 연비 대비 출력 및 기어비 분석을 통해 적재량이 중요한 노선 또는 장거리 노선에는 연비가 다소 낮더라도 출력과 가속 성능이 높은 차량을 배치할 수 있다. 반대로 도심 구간처럼 정지 및 출발이 잦은 환경에서는 연비가 높은 차량을 우선 배치하여 연료비 절감 가능하다.
  - **차량 점검 및 교체 시점 예측**: 차량 무게, 출력, 엔진 구조와 같은 변수를 기반으로 부품 마모 가능성이 높은 차량 사전 식별과 차량 점검 시기 조정을 통해 돌발 고장 및 사고율을 감소시킬 수 있다.

### Q2. 변수들 간의 상관 관계가 높은 조합을 임의로 2개 선택해서 해당 데이터 간의 상관 관계를 그래프로 그리고 어떤 결론을 내릴 수 있는지를 토의하세요.

#### 선택한 상관관계 변수 조합

- 배기량($disp$) - 무게($wt$): `+0.88`의 강한 양의 상관관계
- 무게($wt$) - 연비($mpg$): `-0.86`의 강한 음의 상관관계

<img width="1564" height="632" alt="image" src="https://github.com/user-attachments/assets/58e7701b-f6fd-490c-ad16-25f21edb7a04" />

#### 분석 결론: 매개 효과(Mediation Effect) 확인

분석 결과 **[A]배기량 → [B]무게 → [C]연비**로 이어지는 상관 구조가 형성된다.
통계적으로 이는 A가 B를 통해 C에 영향을 주는 매개 효과(Mediation Effect)를 시사하며,
결과값인 C를 개선하기 위해 직접 변수인 B뿐만 아니라 근본 변수인 A를 제어하는 것도 중요함을 보여준다.

#### 비즈니스 적용 예시

- **엔진 다운사이징의 당위성**: 연비[C] 개선을 위해 단순히 차체를 가볍게 만드는 것(B 개선)은 한계가 있다. 배기량[A]을 낮추되 출력은 유지하는 터보차저 기술 등을 도입하면 무게 감소와 연비 향상을 동시에 달성하는 고부가가치 창출이 가능하다.
- **규제 비용 및 세금 시뮬레이션**: 배기량[A] 기준의 자동차세와 연비[C] 기준의 탄소세를 통합 분석할 수 있다. 기업은 배기량을 소폭 상향할 때 발생하는 세금 변화와 그에 따른 연비 하락으로 인한 과징금 리스크를 사전에 시뮬레이션하여 최적의 수익 구조를 설계할 수 있다.

## 2. W1M2 - SQL Tutorial

### Q1. 각자가 이해하기 어려웠던, 또는 이해하지 못한 keyword에 대해서 함께 토의해 봅시다.

#### 1) SQLite에서 LIKE 문법의 대소문자 구분 불가

**SQLite**의 `LIKE`는 기본적으로 **ASCII case-insensitive**로 동작하도록 설계되어 있다.
따라서 아래의 쿼리는 True를 반환한다.
```sql
SELECT 'a' LIKE 'A'; -- 1 (TRUE)
```

문자열을 비교할 때 일반적으로 대소문자를 구분하는 것으로 생각했지만, 그렇지 않아서 헷갈렸다.
SQLite의 `LIKE` 문법에서 대소문자를 구분하려면 두 가지의 방법이 있다.

##### **방법 1. PRAGMA case_sensitive_like 설정**

아래와 같이 PRAGMA문을 사용하여 대소문자 구분을 활성화할 수 있다.
다만 이 설정은 DEPRECATED 되었으며, [공식 문서](https://sqlite.org/pragma.html#pragma_case_sensitive_like)에서도 사용을 권장하지 않는다.
```sql
PRAGMA case_sensitive_like = TRUE;
```

##### **방법 2. `GLOB` 연산자 사용**

`GLOB` 연산자는 기본적으로 대소문자를 구분한다.
다만, `GLOB`는 와일드카드 문법이 `*`와 `?`로 다르므로 주의가 필요하다.
```sql
SELECT 'a' GLOB 'A'; -- 0 (FALSE)
```

#### 2) SQLite에서 지원하지 않는 문법

SQLite는 경량, 임베디드 데이터베이스로 설계되어 서버형 DB에서 제공하는 일부 확장 문법을 지원하지 않는다.

##### **문자열 패턴 매칭 (LIKE, 와일드카드)**

- `[]` 기반 문자 집합 / 범위 지정은 MSSQL 전용 문법이다.
- SQLite에서는 `OR` 조건으로 직접 나열, 연속 범위인 경우 `BETWEEN` 활용, `GLOB` 연산자로 해결 가능하다.

##### **SELECT INTO 문법**

- SQL Server / MS Access 전용 문법이다.
- SQLite에서는 `CREATE TABLE AS SELECT`로 대체 가능하다. 다만 이 경우 `PK`, `FK`, `INDEX` 등은 복사되지 않는다.

##### **TOP N 문법**

- LIMIT 문법으로 대체 가능하다.

##### **ANY / ALL 문법**

- SQLite는 ANY, ALL을 불필요한 확장 문법으로 판단하여 제외한다. 동일한 의미를 더 단순한 문법으로 표현 가능하다.
```sql
WHERE ProductID IN (
    SELECT ProductID
    FROM OrderDetails
);
```

- `ALL` 의 경우 서브 쿼리 결과에 따라 성립하는 경우가 적어 실무적으로 사용하지 않는다.

##### **날짜 리티럴 문법**

- `#01-01-01#` 형식은 MS Access에서 사용하는 날짜 리터럴이고, SQLITE에서는 인식되지 않는다.
- SQLITE에서는 `'YYYY-MM-DD'` 형식의 문자열을 사용하거나, `DATE()` 함수를 사용하여 날짜를 다룬다.

## 3. W1M3 - ETL 프로세스 구현하기

### Q1. wikipeida 페이지가 아닌, IMF 홈페이지에서 직접 데이터를 가져오는 방법은 없을까요? 어떻게 하면 될까요?

#### 1) 파일 기반 데이터셋 다운로드 방식

- **접근 방식**
  - IMF 데이터 포털에서 제공하는 공식 데이터셋 파일(CSV/Excel)을 직접 다운로드한다. \*\*\*\*
- **구현 방법**
  1. IMF 데이터 포털 내 데이터셋 다운로드 페이지(https://data.imf.org/en/datasets/IMF.RES:WEO)를 HTTP 요청으로 가져온다.
  2. HTML 파싱을 통해 CSV 또는 Excel 형식의 데이터셋 다운로드 링크를 추출한다.
  3. 추출한 링크를 통해 원본 데이터 파일을 다운로드한다.
  4. pandas를 사용해 파일을 로드한 후, 필요한 지표만 필터링하여 DB에 적재한다.
- **장점**
  - 릴리스 시점이 명시되어 있어 데이터 버전 관리가 가능하다.
  - 원본(raw) 파일을 로컬에 보관할 수 있어 ETL 과정의 재현성이 높다.
- **한계 및 고려사항**
  - 필요한 지표 외의 데이터까지 함께 다운로드되므로 후처리 필터링이 필요하다.
  - 다운로드 링크 구조가 변경될 경우 HTML 파싱 로직 수정이 필요하다.
  - 파일 단위 수집 방식이므로 실시간 갱신에는 적합하지 않다.

#### 2) IMF Datamapper 전용 REST API 방식

- **접근 방식**
  - IMF Datamapper 서비스에서 제공하는 REST 형태의 API를 활용하여, 지표 및 국가 목록을 조회하고 지표 단위로 전체 데이터를 수집한다.
- **구현 방법**
  1. IMF Datamapper 공식 문서(https://www.imf.org/external/datamapper/api/help)를 통해 사용 가능한 엔드포인트를 파악한다.
  2. indicators 엔드포인트를 통해 GDP 등 필요한 지표 코드를 확인한다. (예: GDP → NGDPD)
  ```sql
    "NGDPD": {
      "label": "GDP, current prices",
      "description": "Gross domestic product is the most commonly used single measure of a country's overall economic activity. It represents the total value at current prices of final goods and services produced within a country during a specified time period, such as one year.",
      "source": "World Economic Outlook (October 2025)",
      "unit": "Billions of U.S. dollars",
      "dataset": "WEO"
    }
  ```
  3. countries 엔드포인트를 통해 국가 코드 목록을 수집한다.
  ```sql
    "countries": {
    "ABW": {
      "label": "Aruba"
    },
    "AFG": {
      "label": "Afghanistan"
    },
    "AGO": {
      "label": "Angola"
    }
  ```
  4. 지표 코드 기준 엔드포인트를 통해 각 국가 코드별 데이터를 수집한다. GDP의 경우 [https://www.imf.org/external/datamapper/api/v1/NGDPD](https://www.imf.org/external/datamapper/api/v1/NGDPD) 에 각 국가 코드별 GDP 정보가 있다.
  5. 수신한 JSON 데이터를 파싱하여 국가 코드에 매핑한 뒤 DB에 적재한다.

- **장점**
  - 크롤링 대비 안정적이며, API 기반 자동화에 적합하다.
  - JSON 구조가 단순하고 직관적이어서 구현 난이도가 낮다.
- **한계 및 고려사항**
  - 데이터가 갱신된 정확한 시점을 API 응답만으로 파악하기 어렵다.
  - 지표 단위로 전체 데이터를 내려주므로 불필요한 데이터가 포함될 수 있다.

#### 3) SDMX 표준 기반 공식 API 방식

- **접근 방식**
  - IMF가 제공하는 SDMX(Standard Data and Metadata eXchange) 표준 기반 API를 사용하여, 데이터셋, 국가, 지표, 연도 정보를 key 형태로 조합해 필요한 데이터만 조회한다.
- **구현 방법**
  1. IMF SDMX API 문서(https://data.imf.org/en/Resource-Pages/IMF-API)를 참고하여 데이터셋 구조와 SDMX key를 파악한다.
  2. 지표 코드(GDP 등), 국가 코드, 연도 정보를 SDMX Key 형식으로 조합한다.
  3. 조합된 Key를 포함한 API 요청을 통해 Json 또는 XML 응답을 수신한다.
  4. 응답 데이터를 파싱하여 pandas DataFrame으로 변환한 뒤 DB에 적재한다.
- **장점**
  - 국제 표준(SDMX)을 따르므로 타 기관 데이터와 구조적 호환성이 높다.
  - 필요한 데이터만 선택적으로 요청할 수 있어 수집 효율이 높다.
  - 장기적인 ETL 자동화 및 유지보수에 적합하다.
- **단점**
  - SDMX 구조 및 지표 코드, Key 조합 방식에 대한 사전 학습이 필요하다.
  - 초기 구현 난이도가 상대적으로 높다.
  - 응답 구조가 복잡하여 파싱 로직이 길어질 수 있다.

#### 4) IMF 홈페이지 브라우저 요청 분석 기반 데이터 수집

- **접근 방식**
  - IMF 홈페이지에서 사용자가 필터(국가, 연도, 지표 등)를 선택할 때, 브라우저는 내부적으로 특정 API 요청을 전송하고 JSON 형식의 응답을 받아 화면을 구성한다. 이때 네트워크 요청을 분석하여, 동일한 request를 Python 코드로 재현하는 방식이다.
- **구현 방법**
  1. IMF 데이터 포탈 페이지([https://data.imf.org/en/Data-Explorer?datasetUrn=IMF.RES:WEO(9.0.0)](<https://data.imf.org/en/Data-Explorer?datasetUrn=IMF.RES:WEO(9.0.0)>))에 접속한 뒤, 브라우저 개발자 도구를 통해 데이터 조회 시 발생하는 API 요청을 확인한다.
  2. 국가, 연도, 지표 필터 변경에 따라 전송되는 request URL 및 request body 구조를 분석한다.
  3. 분석한 요청 정보를 기반으로 Python에서 동일한 HTTP 요청을 구성한다.
  4. 응답으로 반환되는 JSON 데이터를 파싱하여 필요한 데이터만 정제한 뒤 DB에 적재한다.
- **장점**
  - 브라우저에서 확인되는 데이터와 동일한 결과를 빠르게 확보할 수 있다.
  - 공식 API 구조나 SDMX 표준을 이해하지 않아도 접근이 가능하다.
  - 초기 구현 속도가 빠르며, 프로토타입 또는 단기 분석 목적에 적합하다.
  - UI 기준의 필터 구조를 그대로 활용할 수 있어 구현이 직관적이다.
- **단점**
  - 공식적으로 문서화된 API가 아닌 비공식 접근 방식이다.
  - IMF 내부 구현 변경 시 요청 구조가 쉽게 깨질 수 있다.
  - 장기적인 ETL 파이프라인 운영에는 안정성이 낮다.
  - Request 구조 또는 엔드포인트 변경에 대한 지속적인 대응이 필요하다.
  - 봇 차단 정책 또는 접근 제한이 발생할 가능성이 존재한다.
  - 공식 문서 기반 접근이 아니므로 유지보수 및 재현성 측면에서 불리하다.

### Q2. 만약 데이터가 갱신되면 과거의 데이터는 어떻게 되어야 할까요? 과거의 데이터를 조회하는 게 필요하다면 ETL 프로세스를 어떻게 변경해야 할까요?

#### 데이터 갱신 시 과거 데이터 처리 전략

- **과거 데이터 조회가 필요 없는 경우**: 최신 값 덮어쓰기
  - **방법**: 기존 레코드를 새로운 값으로 `UPDATE` 하거나 테이블을 재생성하여 최신 데이터만 유지
  - **장점**: 구현과 관리가 단순하고 저장 공간을 적게 사용
  - **단점**: 과거 데이터가 사라지므로 시점별 분석이 불가능
- **과거 데이터 조회가 필요한 경우**: 이력 또는 버전 기반 누적 저장
  - **방법**: 데이터가 갱신될 때마다 새로운 레코드를 추가
  - **장점**: 특정 시점의 데이터 조회 가능
  - **단점**: 데이터 양 증가 및 조회 시 필터 조건 복잡도 증가

#### 과거 데이터 조회를 고려한 ETL 프로세스 변경

1. **Extract 단계**
   - 기존 데이터 외에 메타데이터(데이터 출처, 릴리스 시각, ETL 실행 시각)를 함께 수집한다.
2. **Transform / Load 단계**
   - 데이터에 `release_id` , `start_data`, `end_date`, `is_current`와 같이 버전 또는 유효 기간 관련 컬럼을 추가한다.
   - 과거 데이터 조회를 고려하여 덮어쓰기 방식이 아닌 append 방식으로 데이터를 추가한다.
   - 테이블 설계 시에는 과거 데이터가 서로 다른 행으로 저장될 수 있도록 복합 Primary Key를 구성한다.

<img width="1170" height="868" alt="image" src="https://github.com/user-attachments/assets/7627ae44-6385-42c2-822d-fd647307e350" />
