## 1. WordCloud 개요

### 1.1 WordCloud란 무엇인가

WordCloud는 텍스트 데이터에 등장하는 단어들의 빈도(frequency)를 기반으로 각 단어의 상대적 중요도를 시각적으로 표현하는 시각화 기법이다.

```
많이 등장한 단어 → 크게 표시

적게 등장한 단어 → 작게 표시
```

텍스트 데이터의 전반적인 경향과 주요 키워드를 빠르게 파악하는 데 목적이 있다.


## 2. WordCloud의 전체 처리 흐름

WordCloud는 내부적으로 다음과 같은 단계로 생성된다.

```
텍스트 입력
   ↓
토큰화 (단어 분리)
   ↓
단어 빈도 계산
   ↓
단어 크기 스케일링
   ↓
공간 배치 알고리즘
   ↓
이미지 렌더링
```

각 단계는 독립적으로 동작하지만, 앞 단계의 결과가 다음 단계의 입력으로 사용되는 파이프라인 구조를 가진다.

## 3. 단계별 작동 원리

### 3.1 텍스트 입력 단계

```
text = "happy love happy good nice love"
```

WordCloud는 문장 단위가 아닌 하나의 문자열 전체를 입력으로 받는다.

내부적으로 문자열을 순회하며 단어 단위 처리를 수행한다.

### 3.2 토큰화(Tokenization)

기본 동작 방식 : 공백(space)을 기준으로 문자열을 분리

```
["happy", "love", "happy", "good", "nice", "love"]
```

WordCloud의 토큰화 특징
```
	•	형태소 분석 불가
	•	문맥 이해 불가
	•	단순 문자열 분리 기반
```

즉, WordCloud는 자연어 이해(NLP) 모델이 아니라 단순 통계 기반 시각화 도구이다.

### 3.3 단어 빈도 계산

```
{
  "happy": 2,
  "love": 2,
  "good": 1,
  "nice": 1
}
```

- 내부적으로 Counter와 유사한 자료구조를 사용
- 각 단어의 등장 횟수를 집계
- 이 빈도 정보가 WordCloud의 핵심 데이터

### 3.4 단어 크기 스케일링

계산된 단어 빈도를 폰트 크기(font size)로 변환

개념적으로는 다음과 같은 관계를 가짐

```
font_size ∝ word_frequency
```

- 가장 많이 등장한 단어 → 가장 큰 글자
- 나머지 단어들은 상대적 비율로 크기 조정

이 단계에서 다음 옵션들이 영향을 미친다.

```
	•	max_words
	•	min_font_size
	•	max_font_size
```

### 3.5 공간 배치 알고리즘

#### 3.5.1 배치 방식의 특징
```
	•	빈도가 높은 큰 단어부터 우선 배치
	•	이미 배치된 단어와 겹치지 않도록 공간 탐색
	•	빈 공간을 찾을 때까지 반복 시도
```

#### 3.5.2 알고리즘 특성
```
	•	완전한 수학적 최적화 알고리즘 아님
	•	휴리스틱(heuristic) 기반 접근
	•	랜덤 요소 포함
```

이로 인해 동일한 데이터라도 실행할 때마다 단어 위치와 배치가 약간씩 달라질 수 있음 (random_state를 통해 제어 가능)


## 4. WordCloud 옵션과 Sampling의 관계

### 4.1 max_words 옵션

```
WordCloud(max_words=200)
```

- 단어 빈도 기준 상위 N개의 단어만 사용
- 빈도가 낮은 단어는 자동 제거

### 4.2 Sampling 방식

```
random.sample(words, 200)
```

- 원본 데이터 자체를 축소
- 전체 분포를 근사적으로 유지

## 5. WordCloud의 한계와 확장 방향

### 5.1 한계점

```
	- 문맥 이해 불가
	- 부정/긍정 판단 불가
	- 단어 간 관계 표현 불가
	- 불용어(stopwords)에 매우 민감
```

### 5.2 일반적인 확장 방식
```
	- 불용어 제거
	- TF-IDF 기반 단어 가중치
	- N-gram 적용
	- Sentiment 분석 모델과 결합
```