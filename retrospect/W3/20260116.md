# [2026.01.16] 일일 회고

## 1. 오늘의 한 줄 요약
- 3/4주차 미니 프로젝트 계획 재수립

---

## 2. 1일 리뷰
- 미니 프로젝트 관련해서 새로운 시각을 얻게 되었다. 당연히 좋은 아이디어를 픽스를 하고 이를 최종 프로덕트까지 구현해야 한다고 생각했는데, 미니 프로젝트의 목적이 '좋은 데이터 프로덕트 구현'이 아니라 '최종 프로젝트를 위한 연습'이니 굳이 워터폴방식에 매몰될 필요는 없음을 느꼈다. 이 이후에 팀 프로젝트의 방향이 바뀌었다.
- 아직 데이터 프로덕트가 무엇인지, 데이터를 어떻게 보여줘야 할 지, 동일한 데이터라도 보여주는 방식에 따라 그 의미가 크게 달라질 것 같아 이번 미니 프로젝트의 목표를 어찌되었든 '하나의 데이터 프로덕트 구현' 으로 설정했다.
- 이렇게 정하고 나니 비교적 수월하게 전체 프로젝트를 어떻게 진행하고, 어떤 부분에서 고민을 해 볼 것인지가 조금 명확해진 것 같다. 우리 팀은 가장 간단한 버전의 프로덕트를 하나 구현하고 / 그 외에 이 프로덕트를 디벨롭한다면 어떤 기술을 사용할 지, 어떤 아키텍처를 선택할 것인지를 고민하고 피드백을 받아보기로 했다.

- 하둡 아키텍처에 대해 누군가 질문을 한다면 아직 명확하게 대답하지 못할 것 같다. 말로 설명할 수 있을 정도가 되어야 해당 시스템을 온전히 이해한다고 할 수 있을 것 같다. 하둡 아키텍처 다시 공부하자 -> AM, RM, Namenode, Datanode, Container 간의 관계 다시 정리

---

## 3. 회고 (KPT)

### (1) Keep
- 팀원들간 프로젝트 방향 재수립 하면서 되게 많은 대화를 나눴다.(4일동안 대화한 것 보다 오늘 하루 대화량이 더 많았다.) 이번 주 팀 위키 작성하면서도 느꼈는데, 분명 같이 얘기하고 합의된 내용인데 결국 서로 이해한 바가 다른 경우가 있었다. 그래서 오늘 프로젝트 방향 재수립 회의에서는 서로 이해한 바를 다시 본인들의 언어로 풀어가며 확인하는 과정을 거쳤는데, 확실히 오해가 없어지고 의견 합치가 되는 것 같다. -> 팀 결정사항에 대해 본인의 언어로 팀원들에게 다시 설명해서 맞는지 확인하는 과정 거치자

---

### (2) Problem
- 없음

---

### (3) Try
- BI툴 공부하기(스트림릿...? 대시...?)
---

## 4. 오늘의 학습

# MovieLens Hadoop MapReduce 프로젝트

## 1. 초기에 막혔던 부분들

### (1) 두 개의 입력 파일 처리
movies.csv와 ratings.csv를 동시에 입력으로 주었을 때,
Hadoop Streaming에서는 각 파일이 서로 다른 Mapper 인스턴스로 분리되어 처리된다.
이를 구분하기 위해 mapreduce_map_input_file 환경변수를 사용해 파일 이름에 따라 다른 mapper를 실행해야 했다.

### (2) Reduce-side Join 이해 부족
처음에는 movies와 ratings를 어떻게 join해야 할지 몰랐지만,
MapReduce에서는 같은 key(movieId)를 기준으로 Reducer에 모아주는 shuffle 단계가 핵심이다.

그래서 mapper에서 다음과 같은 태그를 붙였다:
- movies: M|title
- ratings: R|rating

Reducer에서는 이 태그를 기준으로 데이터를 구분해 조인하였다.

## 2. Hadoop Streaming에서 중요한 개념

### mapreduce_map_input_file
각 Mapper가 처리 중인 입력 파일의 HDFS 경로가 저장된 환경 변수이다.
이를 사용하면 하나의 Job에서 서로 다른 파일에 대해 다른 Mapper를 실행할 수 있다.

## 3. Hadoop MapReduce 처리 흐름# Amazon Product Reviews MapReduce 프로젝트 회고 (Retrospective)

## 1. 초기 질문과 고민

### Q1. 데이터가 15GB 이상인데 DataNode를 늘리면 속도가 빨라질까?
- Hadoop은 **데이터를 블록 단위로 분산 저장**하고, Map Task를 각 DataNode에서 병렬 실행한다.
- DataNode 수 증가 → 동시에 실행 가능한 Map Task 수 증가 → 처리 시간 단축 가능
- 단, 네트워크/디스크/메모리 병목이 있으면 선형적으로 빨라지지는 않음

**결론**: DataNode 증설은 Map 단계 병렬성을 높이는 데 효과적이지만, 리소스 튜닝이 병행되어야 함

---

### Q2. MapReduce에서 실제로 오래 걸리는 작업은 무엇인가?
- Map 단계: 대용량 CSV 파싱 + key-value 생성
- Shuffle 단계: 네트워크를 통한 대량 데이터 이동
- Reduce 단계: 그룹핑 및 집계

실측 결과:
- Map input records 약 **5억 6천만 건**
- Shuffle bytes 약 **9.5GB**
- 전체 시간 중 Map 단계 비중이 가장 큼

**결론**: 이 작업에서는 Map 단계와 Shuffle 비용이 가장 컸다.

---

## 3. 데이터 업로드 과정에서의 트러블슈팅

### 문제 1. `hdfs dfs -put` 중간에 Killed / Connection refused 발생
**증상**
- 대용량 CSV 업로드 중 일부 파일만 업로드
- DataStreamer 오류 및 특정 DataNode 제외 로그 발생

**원인**
- Docker 환경에서 DataNode JVM 메모리 제한 없음
- HDFS replication 동시 수행으로 메모리 폭증

**해결**
- 초기 업로드 시 `dfs.replication=1`
- 업로드 완료 후 `hdfs dfs -setrep -R 3 /data/amazon`
- Docker `mem_limit` 설정 및 YARN 메모리 제한 추가
---

## 4. Replication 관련 이해 정리

### 질문: replication 값을 나중에 바꿔도 자동으로 복제되는가?
- `dfs.replication` (xml): **미래 파일에만 적용**
- `hdfs dfs -setrep`: **이미 존재하는 파일에 적용**

실제로:
- replication 1 → 3 변경 시
- Under-replicated blocks 증가
- DataNode 간 백그라운드 복제 수행 확인

---

## 5. YARN 장애와 메모리 튜닝

### 문제 2. MapReduce 실행 시 ResourceManager Connection Refused
**증상**
- 8032 포트 연결 실패
- MapReduce 잡 제출 불가

**원인**
- 컨테이너 재기동 이후 YARN 데몬 미기동
- 또는 OOM Killer에 의해 ResourceManager 종료

**해결**
- 전체 컨테이너 재기동
- Docker 메모리 16GB 이상 할당
- YARN/MapReduce 메모리 명시적 제한

```xml
yarn.scheduler.maximum-allocation-mb=2048
yarn.nodemanager.resource.memory-mb=2048
mapreduce.map.memory.mb=512
mapreduce.reduce.memory.mb=512
```

> Docker + Hadoop 환경에서는 메모리 제한이 없으면 반드시 장애가 발생한다.

---

## 6. InvalidResourceRequestException 해결

### 문제 3. Requested memory 1536 > maximum allowed 1024
**원인**
- Map/Reduce 메모리는 줄였으나
- ApplicationMaster(AM) 기본 메모리(1536MB)가 제한 초과

**해결**
- YARN 최대 할당 메모리를 2048MB로 상향
- 또는 AM 메모리 직접 설정

> MapReduce 잡은 Map/Reduce 외에 ApplicationMaster 자원도 고려해야 한다.

---


1. HDFS에서 데이터를 split 단위로 읽음
2. Mapper가 (key, value) 출력
3. Hadoop이 key 기준으로 정렬 및 grouping
4. Reducer가 key별로 값을 받아 집계
5. 결과를 HDFS에 저장


