# [2026.01.14] 일일 회고

## 1. 오늘의 한 줄 요약
- 개인 미션 생각보다 빠르게 진행되어 하둡 시스템 공부할 시간이 확보되어 좋았음
- 팀 아이디어 너무 안나옴

---

## 2. 1일 리뷰
- 개인 미션이 생각보다 빨리 끝났다. M3부터는 하둡 환경 설정보다는 구성한 환경 안에서 mapreduce 작업이 대부분이라 데이터셋만 조금씩 바꾸고, 로직만 조금씩 바꾸면 되어서 그런듯 하다.
조금의 여유가 생겨 하둡 시스템 공부를 다시 꼼꼼하게 했다. 공부하는 과정에서 이전 미션에서 부족한 부분이 보여 수정 작업 중이다. 하둡 config 같은 경우는 image에 함께 copy해서 빌드하기보다는 개별적인 클러스터에서 환경설정이 필요하다고 판단하여 image에는 config파일들을 빼고, docker-compose 단계에서 volume 마운팅으로 개별 config를 적용하는 방안으로 생각 중이다.
하둡 시스템 뭔가 복잡한 듯 하면서도 되게 잘 만들어진 시스템 같다. 다만 이 하둡 시스템도 뭔가 최적화 할 요소들이 있을 것 같은데 데이터셋 크기에 따라 datanode를 얼마나 둘 지, replica를 얼마나 둘 지가 최적화 요소가 되지 않을까 생각한다. 너무 과해도 느려질 것 같고, 너무 적으면 사실 분산 시스템을 사용하는 이유가 없으니 적절한 지점을 찾는게 필요할 것 같다. 이러한 부분이 항상 경험칙에 따르는 것인지, cs적으로 딱 계산으로 떨어지는 문제인지 궁금하다.

- 팀 아이디어가 잘 나오지 않는다. 100만원 이상 지불할 고객...? 아직 100만원 이상의 소비력이 없어서 그런가... 어떤 문제가 100만원씩이나 지불할 의향이 있을까 감이 잘 오지 않는다.

---

## 3. 회고 (KPT)

### (1) Keep
- 없음

---

### (2) Problem
- 팀 회의 타임박싱 안됐음 -> 아이디어가 안나오는데 그래도 타임 박싱하고 넘어가는게 맞나? 라는 의문이 들었음 -> 어차피 더 고민한다고 나올 것 같지도 않음 -> 시간 낭비 -> 타임 박싱 제발 지키자

---

### (3) Try
- 하둡 시스템 최적화 테스트 -> 몇개의 datanode / 몇개의 replica일 때 가장 빠를까?

---

## 4. 오늘의 학습
# Docker 기반 Hadoop 멀티 노드 클러스터 XML 설정

## 1. core-site.xml
Hadoop 클러스터가 사용할 기본 파일시스템(HDFS)의 진입점을 정의한다.  
`fs.defaultFS = hdfs://master:9000` 설정을 통해 모든 Hadoop 명령이 Docker 네트워크 상의 `master` 컨테이너(NameNode)의 9000번 포트로 전달된다.  
이 설정이 없으면 각 컨테이너는 자신의 로컬 파일시스템을 사용하게 되어 클러스터가 성립하지 않는다.

## 2. hdfs-site.xml
HDFS의 실제 저장 구조와 동작 방식을 정의한다.
- `dfs.replication`: 블록 복제 수로, 장애 허용성과 데이터 안정성을 결정한다.
- `dfs.namenode.name.dir`: NameNode의 메타데이터(fsimage, edits) 저장 위치.
- `dfs.datanode.data.dir`: DataNode가 실제 데이터 블록을 저장하는 위치.
- `dfs.datanode.use.datanode.hostname`, `dfs.client.use.datanode.hostname`: Docker 환경에서 IP 대신 hostname을 사용해 컨테이너 재시작 시에도 HDFS가 유지되도록 한다.

이 파일이 없으면 데이터가 분산 저장되지 않거나, 재시작 시 클러스터가 깨질 수 있다.

## 3. mapred-site.xml
MapReduce가 어디에서 실행될지를 결정한다.
- `mapreduce.framework.name = yarn`으로 설정해 MapReduce를 YARN 기반 분산 실행 모드로 전환한다.
- `yarn.app.mapreduce.am.env`, `mapreduce.map.env`, `mapreduce.reduce.env`는 ApplicationMaster, Map, Reduce 컨테이너가 Hadoop 라이브러리를 찾을 수 있도록 환경변수를 제공한다.

이 설정이 없으면 MapReduce는 로컬 JVM에서만 실행되어 분산 처리가 이루어지지 않는다.

## 4. yarn-site.xml
YARN 클러스터의 중앙 스케줄러와 노드 간 통신 방식을 정의한다.
- `yarn.resourcemanager.hostname = master`로 ResourceManager가 실행되는 노드를 지정한다.
- `yarn.nodemanager.aux-services = mapreduce_shuffle` 및 관련 클래스 설정은 Map과 Reduce 사이의 Shuffle 단계가 네트워크로 동작하도록 한다.

이 파일이 없으면 NodeManager들이 ResourceManager에 연결되지 못하거나, Reduce 단계가 Map 결과를 받을 수 없어 작업이 실패한다.

---

이 네 개의 XML 파일이 함께 맞물려 Docker 환경에서 HDFS 기반 분산 저장과 YARN 기반 분산 계산이 가능한 진짜 Hadoop 멀티 노드 클러스터를 구성한다.


# Hadoop Base Dockerfile

## 목적
이 base Dockerfile은 Hadoop 멀티 노드 클러스터에서 모든 노드(master, worker)가 공통으로 사용할 수 있는 실행 환경을 제공하기 위한 이미지이다. Java, Hadoop, SSH, 설정 파일, 디렉토리 구조를 포함한다.

## 주요 구성 요소

### 1. Java & Hadoop 환경
- `FROM eclipse-temurin:8-jdk`: Hadoop과 호환성이 높은 Java 8 기반.
- `HADOOP_HOME`, `JAVA_HOME`, `PATH` 설정으로 모든 Hadoop 명령을 경로 의존성 없이 실행 가능하게 함.

### 2. 필수 패키지
- `openssh-server`: Hadoop 스크립트가 SSH로 다른 노드에 데몬을 띄우기 때문에 필요.
- `rsync`, `netcat-openbsd`: 노드 간 동기화 및 네트워크 디버깅.
- `curl`: Hadoop 바이너리 다운로드.

### 3. Hadoop 설치
Apache 공식 배포판(hadoop-3.3.6)을 다운로드하여 `/opt/hadoop`에 설치해 버전과 경로를 고정한다.

### 4. SSH 무인증 설정
컨테이너 내부에서 root 사용자가 비밀번호 없이 SSH 접속 가능하도록 키를 생성하고 authorized_keys에 등록한다. 이는 `start-dfs.sh`, `start-yarn.sh`가 자동으로 다른 노드에 접속하기 위해 필요하다.

### 5. Hadoop 설정 파일 주입
`core-site.xml`, `hdfs-site.xml`, `mapred-site.xml`, `yarn-site.xml`을 이미지에 포함하여 모든 노드가 동일한 클러스터 설정을 사용하도록 한다.

### 6. HDFS 디렉토리 생성
`/hadoop/dfs/name`과 `/hadoop/dfs/data`를 base 이미지에 모두 생성한다.  
이미지 레벨에서는 두 역할(NameNode, DataNode)을 모두 지원하고, 실제로 어떤 디렉토리를 쓰는지는 docker-compose에서 볼륨 마운트로 결정한다.

### 7. 데몬 실행 사용자
`HDFS_*_USER`, `YARN_*_USER`를 모두 `root`로 설정하여 컨테이너 환경에서 권한 문제로 데몬이 실패하는 상황을 방지한다.


# start-hadoop.sh

## 역할
`start-hadoop.sh`는 Docker 컨테이너가 실행될 때 Hadoop 노드로 동작하도록 만드는 부트스트랩(init) 스크립트이다.  
컨테이너를 단순한 리눅스 환경에서 NameNode 또는 DataNode 역할을 수행하는 클러스터 노드로 전환한다.

---

## 주요 동작 흐름

1. **SSH 데몬 시작**
   - Hadoop의 `start-dfs.sh`, `start-yarn.sh`는 SSH로 다른 노드에 접속해 데몬을 띄우므로 필수.

2. **HADOOP_ROLE에 따라 역할 분기**
   - `master`: NameNode + ResourceManager
   - `worker`: DataNode + NodeManager

3. **NameNode 포맷 조건부 수행**
   - `/hadoop/dfs/name/current`가 없을 때만 `hdfs namenode -format` 실행.
   - Docker 볼륨이 유지될 경우 HDFS 메타데이터를 보호하기 위한 안전장치.

4. **클러스터 서비스 기동**
   - `start-dfs.sh`: HDFS 전체(NameNode, DataNode) 기동
   - `start-yarn.sh`: YARN 전체(ResourceManager, NodeManager) 기동

5. **컨테이너 유지**
   - `tail -f /dev/null`로 PID 1 프로세스를 유지해 컨테이너가 종료되지 않도록 함.

# docker-compose.yml

## 목적
docker-compose.yml은 Hadoop Master(NameNode + ResourceManager)와 여러 Worker(DataNode + NodeManager)를 하나의 가상 네트워크로 묶어 **실제 분산 클러스터처럼 동작**하도록 오케스트레이션한다.

---

## Master 서비스

- `image: hadoop-master`
  - Hadoop base 이미지 위에 Master 역할을 부여한 이미지.
- `container_name` / `hostname: master`
  - HDFS와 YARN 설정에서 사용하는 기준 호스트명.
- `ports`
  - `9870`: HDFS NameNode Web UI
  - `8088`: YARN ResourceManager Web UI
- `volumes`
  - `./data/namenode:/hadoop/dfs/name`
  - NameNode 메타데이터(fsimage, edits)를 호스트 디스크에 저장하여 컨테이너 재시작 시에도 HDFS 상태 유지.

---

## Worker 서비스 (worker1, worker2)

- `image: hadoop-worker`
  - DataNode + NodeManager 역할을 수행.
- `hostname`
  - HDFS와 YARN에서 노드를 식별하는 이름.
- `volumes`
  - `./data/datanode1:/hadoop/dfs/data`
  - `./data/datanode2:/hadoop/dfs/data`
  - 각 DataNode가 **서로 다른 물리 디스크**를 사용하도록 분리.

---

## 네트워크

- `hadoop-net`
  - 모든 컨테이너가 하나의 가상 LAN에 속함.
  - Docker DNS를 통해 `master`, `worker1`, `worker2`로 서로 통신 가능.
  - Hadoop 설정의 hostname 기반 통신과 완전히 일치.

# Hadoop Config

## 핵심 구조
- config/ 는 호스트에서 관리
- docker-compose로 config를 컨테이너에 마운트
- modify-config.py는 호스트에서 실행하여 XML 수정
- Hadoop 서비스는 컨테이너 내부에서 재시작

## 왜 Dockerfile에 COPY config가 있어도 되는가
초기 이미지 기본값 제공용. 실제 운영 값은 volume 마운트가 덮어씀.