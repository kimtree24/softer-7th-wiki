# [2026.01.12] 일일 회고

## 1. 오늘의 한 줄 요약
- Hadoop 시스템에 대한 학습 및 새로운 팀과 그라운드 룰 및 아이디어 회의

---

## 2. 1일 리뷰
하둡 시스템에 대해서 학습했다. 하둡 시스템은 용어만 들어보고 실제로 내부 구조나, 동작 원리 등은 전혀 접해보지 못했기에 생소한 부분이 많았다.

강의 PPT를 다시 읽어보며 정리해야 할 필요가 있을 것 같다.

금주 개인 미션도 읽어보니, 모르는 내용이 너무 많아서 이전보다 학습에 많은 시간이 소요될 듯 하다.

팀 미션 관련해서 중간에 인원 조정이 되어 약간의 혼선이 있었지만 빠르게 팀회의를 진행했다.
이번 주 팀 목표는 최대한 강사님 피드백을 많이 받자는 쪽으로 의견이 모아졌고, 우리 팀 내부에서도 아이디어를 엄청 고도화해서 피드백 받기보다는 가볍게 여러번 회의를 하자는 쪽으로 의견이 모아졌다.

확실히 1,2주차 경험하면서 아이디어 내고 피드백 주고 받는 과정에 익숙해져서였는지 이전보다는 순조롭게 아이디어 회의가 진행된 것 같다.
이러한 부분에서 시간이 세이브 되니 개인 미션 시간오 보장될 것 같아 살짝 기대되는 한 주의 시작이다.


---

## 3. 회고 (KPT)

### (1) Keep
- 팀 회의 중 특정 아이디어에 대한 논의가 길어져서 일단 끊고 디테일한 내용은 다음 회의로 넘기고 전반적인 아이디어 검토를 진행해보자고 했다. time boxing을 지키기 위힌 의견이였고, 너무 하나의 아이디어에 매몰되면 한 번 끊고 가자

---

### (2) Problem
- 팀 미션 아이디어 회의를 하며, 아이디어 검토에 대한 내부 기준 없이 진행해서 초반 아이디어는 세세하게 검토한 반면, 후반부 아이디어는 훑고 넘어가듯이 진행되어버렸다. 회의 안건에 대한 검토 기준을 정하고 회의 진행하자

---

### (3) Try
- 하둡 시스템 구조 온전히 파악하고 과제 진행하자 -> 그냥 과제 진행하면 그냥 코드 따라치기 밖에 안되는 것 같다. -> 내일 오전까지는 하둡 시스템 + 금일 수업내용 복습 + 파악에 중심을 두자

---

## 4. 오늘의 학습 - Hadoop & YARN 학습 정리

### (1) 왜 Hadoop이 만들어졌는가

Hadoop은 극단적인 현실 문제를 해결하기 위해 탄생한 시스템

당시의 현실:

- 데이터는 기하급수적으로 증가
- CPU는 느리고 비쌈
- 대형 서버는 매우 비쌈
- 디스크와 네트워크는 병목

그래서 등장한 질문:

> “데이터가 더럽게 많고, 컴퓨팅이 더럽게 느릴 때, 우리는 무엇을 해야 하는가?”

답:

> “비싼 서버 하나 말고,  
> 고장 나도 되는 싼 서버 여러 대를 써서 나눠 저장하고 계산하자.”

---

### (2) Hadoop의 근본 철학

Hadoop은 세 가지 극단적인 전제를 깔고 설계됨.

#### 1) Cheap Hardware
- 서버는 고장난다 → 기본 전제
- 그래서 데이터를 여러 곳에 복제한다

#### 2) Write Once, Read Many
HDFS는 파일을 수정하지 않는다.

- 한 번 쓰면
- 여러 번 읽는다

이 덕분에:
- 동기화가 필요 없음
- 인덱스 관리 필요 없음
- 구조가 매우 단순해짐 -> **Simple Coherency Model**

#### 3) Moving Computation is Cheaper than Moving Data
> 데이터를 옮기지 말고, 계산을 옮겨라

100GB를 네트워크로 옮기는 것보다  
그 데이터가 있는 서버에서 계산한 결과만 보내는 것이 훨씬 싸다.

이 개념이 Hadoop 전체 구조를 결정

---

### (3) HDFS의 역할과 구조

HDFS는 Hadoop의 스토리지 계층

#### 1) 구성 요소

| 구성요소 | 역할 |
|--------|------|
| NameNode | 파일 메타데이터(어디에 저장됐는지) 관리 |
| DataNode | 실제 데이터 블록 저장 |

NameNode는 데이터를 직접 저장하지 않고,  
DataNode가 실제 파일 내용을 보관한다.

---

#### 2) 장애 허용 (Fault Tolerance)

서버는 망가진다 → 전제

그래서:
- 같은 데이터를 여러 DataNode에 복제
- NameNode는 “어디에 몇 개 있는지” 추적

---

### (4) Rack과 Data Locality

#### 1) Rack
Rack은 물리적으로 가까운 서버 묶음이다.

| 위치 | 비용 |
|------|------|
| Same Node | 매우 빠름 |
| Same Rack | 빠름 |
| Different Rack | 매우 느림 |

Hadoop은 항상 같은 노드 → 같은 랙 → 다른 랙 순으로 작업 배치

---

#### 2) Data Locality

Data Locality는 데이터가 있는 곳에서 연산이 실행되는 것

Hadoop은 task를 배치할 때:
- 해당 데이터 블록이 있는 DataNode에 먼저 시도
- 네트워크 이동 최소화

---

### (5) YARN — Hadoop의 운영체제

HDFS가 **스토리지**라면  
YARN은 **클러스터 자원 관리 OS**다.

---

#### 1) YARN 구성

| 구성요소 | 역할 |
|--------|------|
| ResourceManager (RM) | 전체 클러스터 자원 관리 |
| NodeManager (NM) | 각 노드의 CPU·메모리·컨테이너 관리 |
| ApplicationMaster (AM) | 하나의 작업(MapReduce Job) 관리자 |
| ApplicationManager | AM 생성 및 상태 관리 |

---

#### 2) YARN 실행 흐름
```
Client -> ResourceManager -> ApplicationMaster 생성 -> AM이 RM에 컨테이너 요청 -> RM이 NodeManager에 컨테이너 할당 -> Task 실행 -> 결과가 AM으로 보고
```

YARN은 이 과정에서 **Data Locality를 고려하여** 컨테이너를 배치한다.

---

### (6) MapReduce와 HDFS

MapReduce는 데이터를 나눠 처리하고 다시 합치는 계산 모델

#### 1) 실행 특징

- 입력: HDFS
- 출력: HDFS
- 중간 결과(Shuffle 단계)는 로컬 디스크

이유 -> HDFS는 write-once이기 때문에 중간 결과를 수정할 수 없기 때문

---

#### 2) Shuffle의 비용

Shuffle은 Mapper 결과를 Reducer에게 네트워크로 보내는 단계

Rack을 넘으면 매우 느리기 때문에:
- Data Locality
- Rack Locality
가 매우 중요하다.

---

### (7) Docker 기반 단일 노드 Hadoop

Docker 위에 Hadoop을 설치한 것이 아니라 **Docker 위에 분산 스토리지 서버(HDFS)를 띄운 것** 이다.

구조:

```
HDFS (NameNode, DataNode)
↓
Docker Container
↓
Docker Volume
↓
Host Disk
```

- HDFS는 컨테이너 안에서 실행
- 실제 데이터는 호스트 디스크에 영속 저장