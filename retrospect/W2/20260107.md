# [2026.01.07] 일일 회고

## 1. 오늘의 한 줄 요약
> 어제 팀원들과 확정지은 아이디어에 대해 수행해 본 결과 데이터셋 확보가 되지 않아 오전에 다시 팀 회의 진행하여 아이디어 확정 후 데이터셋 확보하였음

---

## 2. 1일 리뷰

### (1) 수행 내역
- 팀원들과 아이디어 피보팅 후 새로운 아이디어 확정
- 팀 아이디어를 바탕으로 크롤러 구현
- 수집한 데이터를 최소한의 전처리 및 불용성 단어 제거 후 wordcloud 구현

### (2) 결과 / 산출물
- 어제 확정지은 아이디어(나라 장터 기반 입찰가 정보 제공)의 데이터 수집시 발생한 문제(동적 html 크롤링 + uuid 기반 html태그)를 팀원들과 공유
- 새로운 아이디어(웨딩 관련 정보 제공)를 제공하고 어떤 데이터를 어디서 수집할 지 의논(스드메 정보, 예식장, 상견례 정보)
- 크롤러 구성하며 구글의 봇 정책 우회하기 위해 셀레니움 도입
- wordcloud 구현하며 불용성 단어 제거하지 않을 경우 의미 있는 결과 도출 어려움을 깨달음

---

## 3. 회고 (KPT)

### (1) Keep
- 팀에서 나온 아이디어를 집가서 빠르게 구현 가부 확인해본 것이 좋았음. 이를 통해 오전에 팀원들 모이자마자 문제 공유 및 해결 방안 수립이 가능했음. 아이디어는 구현 과정에서 엎어질 수 있으니, 아이디어 디밸롭에 너무 많은 시간을 투자하기 보다는 실제로 구현 단계로 넘어가서 최소한의 검증을 빠르게 하자. (아이디어 확정 후 팀원들끼리 각자 검증하는 단계로 바로 나아가자)

---

### (2) Problem
- 어제 집 가서도 팀 미션 진척도가 너무 낮아 미션 수행을 진행 했었음. 이로 인해 금일 피로도가 누적되고 컨디션이 너무 좋지 않아서 예민했던 것 같음. 너무 늦게까지는 작업을 하지 말자.(00시에는 무조건 컴퓨터 끄자) 다음날 일과 중에 조금 더 빠듯하게 진행하거나 팀원들의 도움을 받는 것이 더 효율적일 것 같음.

---

### (3) Try
- 없음

---

## 4. 오늘의 학습

### (1) M5 - WordCloud

WordCloud는 텍스트 데이터에 등장하는 단어들의 빈도(frequency)를 기반으로 각 단어의 상대적 중요도를 시각적으로 표현하는 시각화 기법이다.

```
많이 등장한 단어 → 크게 표시

적게 등장한 단어 → 작게 표시
```

텍스트 데이터의 전반적인 경향과 주요 키워드를 빠르게 파악하는 데 목적이 있다.


### (2) WordCloud의 전체 처리 흐름

WordCloud는 내부적으로 다음과 같은 단계로 생성된다.

```
텍스트 입력
   ↓
토큰화 (단어 분리)
   ↓
단어 빈도 계산
   ↓
단어 크기 스케일링
   ↓
공간 배치 알고리즘
   ↓
이미지 렌더링
```

각 단계는 독립적으로 동작하지만, 앞 단계의 결과가 다음 단계의 입력으로 사용되는 파이프라인 구조를 가진다.

### (3) 단계별 작동 원리

#### 1) 텍스트 입력 단계

```
text = "happy love happy good nice love"
```

WordCloud는 문장 단위가 아닌 하나의 문자열 전체를 입력으로 받는다.

내부적으로 문자열을 순회하며 단어 단위 처리를 수행한다.

#### 2) 토큰화(Tokenization)

기본 동작 방식 : 공백(space)을 기준으로 문자열을 분리

```
["happy", "love", "happy", "good", "nice", "love"]
```

WordCloud의 토큰화 특징
```
	•	형태소 분석 불가
	•	문맥 이해 불가
	•	단순 문자열 분리 기반
```

즉, WordCloud는 자연어 이해(NLP) 모델이 아니라 단순 통계 기반 시각화 도구이다.

#### 3) 단어 빈도 계산

```
{
  "happy": 2,
  "love": 2,
  "good": 1,
  "nice": 1
}
```

- 내부적으로 Counter와 유사한 자료구조를 사용
- 각 단어의 등장 횟수를 집계
- 이 빈도 정보가 WordCloud의 핵심 데이터

#### 4) 단어 크기 스케일링

계산된 단어 빈도를 폰트 크기(font size)로 변환

개념적으로는 다음과 같은 관계를 가짐

```
font_size ∝ word_frequency
```

- 가장 많이 등장한 단어 → 가장 큰 글자
- 나머지 단어들은 상대적 비율로 크기 조정

이 단계에서 다음 옵션들이 영향을 미친다.

```
	•	max_words
	•	min_font_size
	•	max_font_size
```

#### 5) 공간 배치 알고리즘

#### i) 배치 방식의 특징
```
	•	빈도가 높은 큰 단어부터 우선 배치
	•	이미 배치된 단어와 겹치지 않도록 공간 탐색
	•	빈 공간을 찾을 때까지 반복 시도
```

#### ii) 알고리즘 특성
```
	•	완전한 수학적 최적화 알고리즘 아님
	•	휴리스틱(heuristic) 기반 접근
	•	랜덤 요소 포함
```

이로 인해 동일한 데이터라도 실행할 때마다 단어 위치와 배치가 약간씩 달라질 수 있음 (random_state를 통해 제어 가능)


### (4) WordCloud 옵션과 Sampling의 관계

#### 1) max_words 옵션

```
WordCloud(max_words=200)
```

- 단어 빈도 기준 상위 N개의 단어만 사용
- 빈도가 낮은 단어는 자동 제거

#### 2) Sampling 방식

```
random.sample(words, 200)
```

- 원본 데이터 자체를 축소
- 전체 분포를 근사적으로 유지

### (5) WordCloud의 한계와 확장 방향

#### 1) 한계점

```
	- 문맥 이해 불가
	- 부정/긍정 판단 불가
	- 단어 간 관계 표현 불가
	- 불용어(stopwords)에 매우 민감
```

#### 2) 일반적인 확장 방식
```
	- 불용어 제거
	- TF-IDF 기반 단어 가중치
	- N-gram 적용
	- Sentiment 분석 모델과 결합
```